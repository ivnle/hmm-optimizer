# Neural reparameterization configuration - MLP generates HMM parameters
# Uses Chiu & Rush 2020 architecture without LVD

model:
  n_hidden_states: 512
  tokenizer: "meta-llama/Llama-2-7b-hf"

training:
  learning_rate: 0.04  # AdamW learning rate
  weight_decay: 0.1  # AdamW weight decay
  batch_size: 64
  max_seq_length: 64
  n_epochs: 20
  param_std: 1.0  # HMM parameter initialization std (ignored for reparameterized model)

reparameterization:
  hidden_dim: 128
  n_layers: 2  # Number of layers in Chiu & Rush MLP (default: 2)